{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0dfe75",
   "metadata": {},
   "source": [
    "# List of libraries to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98a1e97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyprind\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mpickle\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import gradio as gr\n",
    "\n",
    "import pyprind, os, sys, re, requests, nltk, math, pickle, gzip\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from collections import defaultdict\n",
    "\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "from matplotlib.widgets import Cursor\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f1557",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5c44a",
   "metadata": {},
   "source": [
    "# Scraping Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565c24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to fetch the page. Status code: 401\n"
     ]
    }
   ],
   "source": [
    "def scrape_website(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'})\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the data you need using BeautifulSoup methods\n",
    "        # For example, let's extract all the text from paragraph (p) tags\n",
    "        paragraphs = soup.find_all('p')\n",
    "        extracted_data = [paragraph.get_text() for paragraph in paragraphs]\n",
    "        return extracted_data\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch the page. Status code: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b5e1c",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1abfbd2",
   "metadata": {},
   "source": [
    "# Scraping using HTML class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b344cf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GET_TEXT(url, target_class=None):\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()  # Raise an error for unsuccessful requests\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"0\"\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find elements with the specified class\n",
    "    elements = soup.find_all(class_=target_class)\n",
    "\n",
    "    if not elements:\n",
    "        return \"0\"\n",
    "\n",
    "    # Extract and return text content\n",
    "    Result = [element.get_text(strip=True) for element in elements]\n",
    "    \n",
    "    if Result:\n",
    "        return Result\n",
    "    else:\n",
    "        return []\n",
    "GET_TEXT(\"https://www.reuters.com/world/europe/death-toll-german-christmas-market-car-ramming-rises-four-bild-reports-2024-12-21/\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cb996b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\conda_tmp\\ipykernel_15888\\1520141605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# for data processing and .csv I/O\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;31m# for progress bars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnewspaper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArticle\u001b[0m \u001b[1;31m# for parsing Reuters articles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m \u001b[1;31m# for web scraping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;31m# for getting today's date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59ffdeb2",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a63e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url(input_string):\n",
    "    try:\n",
    "        url_pattern = re.compile(\n",
    "            r'^(https?://)?'                      # Optional scheme (http or https)\n",
    "            r'((([a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,})|'  # Domain name\n",
    "            r'localhost|'                          # OR localhost\n",
    "            r'(\\d{1,3}\\.){3}\\d{1,3})'              # OR IPv4 address\n",
    "            r'(:\\d+)?'                             # Optional port\n",
    "            r'(/[-a-zA-Z0-9@:%._+~#=]*)*'          # Optional path\n",
    "            r'(\\?[;&a-zA-Z0-9%._+~#=-]*)?'         # Optional query\n",
    "            r'(#[-a-zA-Z0-9_]*)?$'                 # Optional fragment\n",
    "        )\n",
    "        return bool(url_pattern.match(input_string))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa1644",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a461b",
   "metadata": {},
   "source": [
    "# Scraping date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01a01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_date(date_str):\n",
    "    try:\n",
    "        # Try parsing the string into a datetime object\n",
    "        parse(date_str)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        # If parsing fails, the string is not a valid date\n",
    "        return False\n",
    "    \n",
    "def extract_elements_by_class(url, target_class):\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()  # Raise an error for unsuccessful requests\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"0\"\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find elements with the specified class\n",
    "    elements = soup.find_all(class_=target_class)\n",
    "\n",
    "    if not elements:\n",
    "        return \"0\"\n",
    "\n",
    "    # Extract and return text content\n",
    "    Result = [element.get_text(strip=True)[:17] for element in elements]\n",
    "    \n",
    "    if is_valid_date(Result[0]):\n",
    "        return Result[0]\n",
    "    else:\n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341a43c",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8423e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publication_date(url):\n",
    "    try:\n",
    "        # Send HTTP request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Common metadata tags for publication dates\n",
    "        date_meta_tags = [\n",
    "            {\"name\": \"pubdate\"}, {\"name\": \"publish-date\"}, {\"name\": \"creation-date\"}, \n",
    "            {\"name\": \"date\"},{\"property\": \"article:published_time\"},\n",
    "            {\"property\": \"og:article:published_time\"} ]\n",
    "\n",
    "        # Search for publication date in meta tags\n",
    "        for tag in date_meta_tags:\n",
    "            date_tag = soup.find(\"meta\", tag)\n",
    "            if date_tag and date_tag.get(\"content\"):\n",
    "                return date_tag['content'][:10]\n",
    "\n",
    "        # Look for visible date patterns in the page content\n",
    "        possible_date_tags = soup.find_all([\"time\", \"span\", \"p\"])\n",
    "        for tag in possible_date_tags:\n",
    "            if tag.has_attr(\"datetime\"):  # For <time> tags\n",
    "                return tag['datetime'][:10]\n",
    "            elif \"published\" in tag.get(\"class\", []) or \"date\" in tag.get(\"class\", []):\n",
    "                return tag.text.strip()[:10]\n",
    "\n",
    "        # Fallback: Search for recognizable date formats in text (not always accurate)\n",
    "        text_content = soup.get_text()\n",
    "        date_patterns = [  r'\\b\\d{4}-\\d{2}-\\d{2}\\b',          # Format: YYYY-MM-DD\n",
    "                           r'\\b\\d{2}/\\d{2}/\\d{4}\\b',          # Format: MM/DD/YYYY\n",
    "                           r'\\b\\d{1,2} \\w{3,9} \\d{4}\\b']     # Format: DD Month YYYY\n",
    "\n",
    "        for pattern in date_patterns:\n",
    "            match = re.search(pattern, text_content)\n",
    "            if match:\n",
    "                return match.group()[:10]\n",
    "\n",
    "        return \"0\"\n",
    "    except requests.RequestException as e:\n",
    "        return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d1f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f0fec99",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636ae7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_as_date_type(text):\n",
    "    # Regex pattern for Month DD, YYYY\n",
    "    date_pattern = r'\\b(\\w+)\\s(\\d{1,2}),\\s(\\d{4})\\b'\n",
    "    match = re.search(date_pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        # Extract month, day, year\n",
    "        month_name, day, year = match.groups()\n",
    "        # Convert to datetime.date\n",
    "        date_obj = datetime.strptime(f\"{month_name} {day} {year}\", \"%B %d %Y\").date()\n",
    "        return date_obj\n",
    "    return \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81abff",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c1d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates_from_text(text):\n",
    "    # Pattern to capture potential date-like segments in the text\n",
    "    date_pattern = r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}|\\w+\\s\\d{1,2},\\s\\d{4})\\b'\n",
    "    \n",
    "    # Find all matches\n",
    "    potential_dates = re.findall(date_pattern, text)\n",
    "    extracted_dates = []\n",
    "    \n",
    "    for date_str in potential_dates:\n",
    "        try:\n",
    "            # Parse the date string to a datetime object\n",
    "            parsed_date = parse(date_str, fuzzy=True).date()\n",
    "            extracted_dates.append(parsed_date)\n",
    "        except ValueError:\n",
    "            # Skip strings that can't be parsed as dates\n",
    "            continue\n",
    "    \n",
    "    return extracted_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596143c",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "469b6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date_str):\n",
    "    # If the input is a list containing a datetime.date object, extract the date and format it\n",
    "    if isinstance(date_str, list) and isinstance(date_str[0], date):\n",
    "        return date_str[0].strftime('%Y-%m-%d')\n",
    "\n",
    "    # Try parsing the date with the '%Y-%m-%d' format first\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        return date_obj.strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    # Try parsing the date with the other formats and convert it to '%Y-%m-%d'\n",
    "    formats = ['%m/%d/%Y', '%d-%b-%y', '%d-%b-%Y', '%a, %b %d, %Y',\n",
    "               '%B %d, %Y,',\"%B %d, %Y\", '%a, %b %d, %Y,', \"%b %d, %Y\", \"%b %d, %Y,\",\n",
    "              \"%d-%b-%y\", \"%a, %b %d, %Y, %I:%M %p\", \"%a, %b %d, %Y\"]\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, fmt)\n",
    "            return date_obj.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # If none of the formats match, return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1cc7e",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7d785",
   "metadata": {},
   "source": [
    "# Add New Links to scrapping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0f3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Links = [\"\"]\n",
    "\n",
    "Links = pd.read_csv(\"News_Links.csv\")\n",
    "update_links = {\"Links\": []}\n",
    "\n",
    "for i in New_Links:\n",
    "    if i not in Links[\"Links\"]:\n",
    "        update_links[\"Links\"].append(i)\n",
    "        \n",
    "New_Links = pd.DataFrame(update_links)\n",
    "New_Links = pd.concat([Links, New_Links]).drop_duplicates(subset=\"Links\").dropna()\n",
    "New_Links.to_csv(\"News_Links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a09b758",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849315c8",
   "metadata": {},
   "source": [
    "# Extract Date of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c28eadac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 11931/11931 [15:05<00:00, 13.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the existing data from the CSV\n",
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "\n",
    "# Define a function to extract dates based on various conditions\n",
    "def extract_dates(Text):\n",
    "    for i in tqdm(range(len(Text)), ncols=100):\n",
    "        try:\n",
    "            link = Text[\"Link\"][i]\n",
    "            current_date = Text[\"Date\"][i]\n",
    "            \n",
    "            # Proceed if the date is not already filled\n",
    "            if current_date == \"0\" or np.isnan(current_date):\n",
    "                # Extract date for ABC News\n",
    "                if \"abcnews\" in link:\n",
    "                    Text.loc[i, \"Date\"] = extract_elements_by_class(link, \"VZTD mLASH gpiba\".strip())\n",
    "                \n",
    "                # Extract date for Yahoo Finance\n",
    "                elif \"yahoo\" in link:\n",
    "                    Text.loc[i, \"Date\"] = extract_elements_by_class(link, \"byline-attr-meta-time\".strip())\n",
    "                \n",
    "                # Extract date for CNN News\n",
    "                elif \"cnn\" in link:\n",
    "                    timestamp = GET_TEXT(link, \"timestamp vossi-timestamp\")\n",
    "                    Text.loc[i, \"Date\"] = extract_date_as_date_type(timestamp[0]) if timestamp else \"0\"\n",
    "                \n",
    "                # Extract date for CoinDesk\n",
    "                elif \"coindesk\" in link:\n",
    "                    content = GET_TEXT(link, \"Noto_Sans_xs_Sans-400-xs flex gap-4 text-charcoal-600 flex-col md:flex-row\")[0][:12]\n",
    "                    Text.loc[i, \"Date\"] = content\n",
    "                \n",
    "                # Extract date for Tesls\n",
    "                elif \"teslarati\" in link:\n",
    "                    content = GET_TEXT(link, \"post-date updated\")[0]\n",
    "                    Text.loc[i, \"Date\"] = content\n",
    "                elif \"cnbc\" in link:\n",
    "                    content = get_publication_date(link)\n",
    "                    Text.loc[i, \"Date\"] = content\n",
    "\n",
    "                # Retain existing date if no conditions met\n",
    "                else:\n",
    "                    Text.loc[i, \"Date\"] = current_date\n",
    "\n",
    "        except Exception as e:\n",
    "            pass  # Log or handle the exception if needed\n",
    "\n",
    "    return Text\n",
    "\n",
    "\n",
    "\n",
    "# Update the DataFrame with the extracted dates\n",
    "Text = extract_dates(Text)\n",
    "\n",
    "# Save the updated data back to CSV\n",
    "Text.to_csv(\"News_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9a475",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c57215",
   "metadata": {},
   "source": [
    "# Convert date to same formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8ec7ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 11931/11931 [00:01<00:00, 6420.06it/s]\n"
     ]
    }
   ],
   "source": [
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "for i in tqdm(range(len(Text)), ncols=100):\n",
    "    try:\n",
    "        Text.loc[i, \"Date\"] = convert_date(Text[\"Date\"][i])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "Text.to_csv(\"News_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f94d42",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e4774",
   "metadata": {},
   "source": [
    "# Remove duplicates patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternExtractor:\n",
    "    def __init__(self, file_path, min_words=10, occurrence_threshold=4):\n",
    "        self.file_path = file_path\n",
    "        self.min_words = min_words\n",
    "        self.occurrence_threshold = occurrence_threshold\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads dataset from CSV file.\"\"\"\n",
    "        return pd.read_csv(self.file_path)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Cleans text by removing extra spaces, punctuation, and converting to lowercase.\"\"\"\n",
    "        text = re.sub('<[^>]*>', '', str(text))\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\(|\\)|D|P)', text)\n",
    "        text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "        return text.lower()\n",
    "\n",
    "    def extract_patterns_from_text(self, text):\n",
    "        \"\"\"Extracts patterns from a single article.\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        patterns = set()\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            for i in range(len(words) - self.min_words + 1):\n",
    "                patterns.add(' '.join(words[i:i + self.min_words]))\n",
    "        return patterns\n",
    "\n",
    "    def find_common_patterns(self):\n",
    "        \"\"\"Finds common patterns across articles.\"\"\"\n",
    "        pattern_counts = defaultdict(int)\n",
    "        for article in tqdm(self.data[\"Text\"], desc=\"Processing Articles\"):\n",
    "            preprocessed_text = self.preprocess_text(article)\n",
    "            patterns = self.extract_patterns_from_text(preprocessed_text)\n",
    "            for pattern in patterns:\n",
    "                pattern_counts[pattern] += 1\n",
    "        return {pattern: count for pattern, count in pattern_counts.items() if count > 1}\n",
    "\n",
    "    def remove_patterns_from_text(self, text, patterns):\n",
    "        \"\"\"Removes patterns from text.\"\"\"\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(re.escape(pattern), \"\", text)\n",
    "        return text\n",
    "\n",
    "    def update_data(self):\n",
    "        \"\"\"Updates data by removing high-frequency patterns.\"\"\"\n",
    "        common_patterns = self.find_common_patterns()\n",
    "        high_frequency_patterns = [pattern for pattern, count in common_patterns.items() if count > self.occurrence_threshold]\n",
    "        self.data[\"Text\"] = self.data[\"Text\"].apply(lambda x: self.remove_patterns_from_text(x, high_frequency_patterns))\n",
    "        self.data.to_csv(\"News_Data_Updated.csv\", index=False)\n",
    "        print(\"Updated News_Data_Updated.csv saved successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nltk.download('punkt')\n",
    "    extractor = PatternExtractor(\"News_Data.csv\")\n",
    "    extractor.update_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93e67f",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d43ec",
   "metadata": {},
   "source": [
    "# Using FinBert model for sentiment texts and label it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356475fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama Barakat\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Osama Barakat\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "100%|█████████████████████████████████████████████████████████| 11931/11931 [09:34<00:00, 20.78it/s]\n"
     ]
    }
   ],
   "source": [
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = softmax(outputs.logits, dim=1)\n",
    "    sentiment_labels = [\"neutral\", \"positive\", \"negative\"]\n",
    "    return sentiment_labels[probabilities.argmax()]\n",
    "\n",
    "for i in tqdm(range(len(Text[\"Sentiment\"])), ncols=100):\n",
    "    try:\n",
    "        np.isnan(Text[\"Sentiment\"][i])\n",
    "        Text.loc[i, \"Sentiment\"] = get_sentiment(Text[\"Text\"][i])\n",
    "    except Exception as e:\n",
    "        pass\n",
    "# Initialize LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "# Encode sentiment column\n",
    "Text[\"Labeling_Sentiment\"] = encoder.fit_transform(Text[\"Sentiment\"])\n",
    "Text.to_csv(\"News_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b398a",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93053f76",
   "metadata": {},
   "source": [
    "# Remove duplicates, empity, and zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d58b5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "\n",
    "# Initialize an empty dictionary to store unique data\n",
    "Refresh = {\"Link\": [], \"Text\": [], \"Sentiment\": [], \"Date\": [], \"Labeling_Sentiment\": []}\n",
    "\n",
    "# Use the 'drop_duplicates' function for unique rows based on 'Link' and 'Text'\n",
    "Text_filtered = Text.drop_duplicates(subset=[\"Link\", \"Text\"])\n",
    "\n",
    "# Filter rows where the Link is valid (is_url returns True)\n",
    "Text_filtered = Text_filtered[Text_filtered[\"Link\"].apply(is_url).fillna(False)]\n",
    "\n",
    "# Drop rows where Text[\"Text\"] == \"0\"\n",
    "Text_filtered = Text_filtered[Text_filtered[\"Text\"] != \"0\"]\n",
    "Text_filtered = Text_filtered[Text_filtered[\"Date\"] != \"0\"]\n",
    "Text_filtered = Text_filtered[Text_filtered[\"Date\"] != None]\n",
    "\n",
    "\n",
    "# Append the filtered rows into Refresh dictionary\n",
    "Refresh[\"Link\"] = Text_filtered[\"Link\"].tolist()\n",
    "Refresh[\"Text\"] = Text_filtered[\"Text\"].tolist()\n",
    "Refresh[\"Sentiment\"] = Text_filtered[\"Sentiment\"].tolist()\n",
    "Refresh[\"Date\"] = Text_filtered[\"Date\"].tolist()\n",
    "Refresh[\"Labeling_Sentiment\"] = Text_filtered[\"Labeling_Sentiment\"].tolist()\n",
    "\n",
    "# Create DataFrame from the dictionary\n",
    "New_Data = pd.DataFrame(Refresh).dropna()\n",
    "\n",
    "# Save the filtered and updated DataFrame\n",
    "New_Data.to_csv(\"News_Data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9853ce",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c96a87",
   "metadata": {},
   "source": [
    "# Training Byte-Pair Encoding: Subword-based tokenization algorithm with saveing progress in BPE file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45c1eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    \"\"\"Byte-Pair Encoding: Subword-based tokenization algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, vocab_size):\n",
    "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # pre-tokenize the corpus into words, BERT pre-tokenizer is used here\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "    \n",
    "    \n",
    "    def train(self, checkpoint_path=None, checkpoint_interval=10):\n",
    "        \"\"\"Train BPE tokenizer with checkpoint saving.\"\"\"\n",
    "        # compute the frequencies of each word in the corpus\n",
    "        for text in self.corpus:\n",
    "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "            new_words = [word for word, offset in words_with_offsets]\n",
    "            for word in new_words:\n",
    "                self.word_freqs[word] += 1\n",
    "\n",
    "        # compute the base vocabulary of all characters in the corpus\n",
    "        alphabet = []\n",
    "        for word in self.word_freqs.keys():\n",
    "            for letter in word:\n",
    "                if letter not in alphabet:\n",
    "                    alphabet.append(letter)\n",
    "        alphabet.sort()\n",
    "\n",
    "        # add the special token </w> at the beginning of the vocabulary\n",
    "        vocab = [\"</w>\"] + alphabet.copy()\n",
    "\n",
    "        # split each word into individual characters before training\n",
    "        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n",
    "\n",
    "        # merge the most frequent pair iteratively until the vocabulary size is reached\n",
    "        iteration = 0\n",
    "        while len(vocab) < self.vocab_size:\n",
    "            # compute the frequency of each pair\n",
    "            pair_freqs = self.compute_pair_freqs()\n",
    "\n",
    "            # find the most frequent pair\n",
    "            best_pair = \"\"\n",
    "            max_freq = None\n",
    "            for pair, freq in pair_freqs.items():\n",
    "                if max_freq is None or max_freq < freq:\n",
    "                    best_pair = pair\n",
    "                    max_freq = freq\n",
    "\n",
    "            # merge the most frequent pair\n",
    "            self.splits = self.merge_pair(*best_pair)\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            vocab.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "            # Save a checkpoint\n",
    "            iteration += 1\n",
    "            if checkpoint_path and iteration % checkpoint_interval == 0:\n",
    "                self.save_checkpoint(checkpoint_path, iteration)\n",
    "        return self.merges\n",
    "\n",
    "    def save_checkpoint(self, filepath, iteration):\n",
    "        \"\"\"Save the current training state as a checkpoint.\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'merges': self.merges,\n",
    "                'splits': self.splits,\n",
    "                'word_freqs': self.word_freqs,\n",
    "                'iteration': iteration,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'corpus': self.corpus,\n",
    "            }, f)\n",
    "        print(f\"Checkpoint saved at iteration {iteration} to {filepath}\")\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"Compute the frequency of each pair.\"\"\"\n",
    "\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for word, freq in self.word_freqs.items():\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i + 1])\n",
    "                pair_freqs[pair] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "\n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"Merge the given pair.\"\"\"\n",
    "\n",
    "        for word in self.word_freqs:\n",
    "            split = self.splits[word]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            self.splits[word] = split\n",
    "        return self.splits\n",
    "    \n",
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "bpe = BPE(corpus= Text[\"Text\"], vocab_size=100000)\n",
    "bpe.train(checkpoint_path=\"BPE.pkl\", checkpoint_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0953cc",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cae226",
   "metadata": {},
   "source": [
    "# Logistic regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b56fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1200/1200 [1:24:38<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "F1-Score: 0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "Text = pd.read_csv(\"News_Data.csv\")[:1000]\n",
    "\n",
    "# Load the saved checkpoint (dictionary)\n",
    "with open(\"BPE.pkl\", \"rb\") as f:\n",
    "    checkpoint = pickle.load(f)\n",
    "\n",
    "# Extract necessary data from the checkpoint\n",
    "merges = checkpoint['merges']\n",
    "splits = checkpoint['splits']\n",
    "word_freqs = checkpoint['word_freqs']\n",
    "vocab_size = checkpoint['vocab_size']\n",
    "corpus = checkpoint['corpus']\n",
    "\n",
    "\n",
    "# Manually handle the tokenization process based on the loaded dictionary\n",
    "def tokenize_with_bpe(text, merges, splits):\n",
    "    # Pre-tokenize the text (splitting into words)\n",
    "    pre_tokenized_text = text.split()  # This is a simple split, you can modify to use any tokenizer\n",
    "    splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "    # Merge based on the merges dictionary\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits_text):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits_text[idx] = split\n",
    "\n",
    "    # Return the merged text (flattened list)\n",
    "    result = sum(splits_text, [])\n",
    "    return result\n",
    "\n",
    "# Sample text data and labels\n",
    "text_data = Text[\"Text\"]\n",
    "\n",
    "# Tokenize the text using the dictionary-based BPE\n",
    "tokenized_data = [\" \".join(tokenize_with_bpe(text, merges, splits)) for text in tqdm(text_data, ncols=100)]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(tokenized_data)\n",
    "y = Text[\"Labeling_Sentiment\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on Training data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_training = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_training:.2f}\")\n",
    "\n",
    "f1_training = f1_score(y_test, y_pred, average=\"weighted\")  # Use 'weighted' for multi-class\n",
    "print(f\"F1-Score: {f1_training:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07cd60d",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9850fd5",
   "metadata": {},
   "source": [
    "# Display Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa07caf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwidgets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cursor\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Function to calculate RSI\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_rsi\u001b[39m(data, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Cursor\n",
    "import gradio as gr\n",
    "\n",
    "# Function to calculate RSI\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff(1)\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Function to analyze stock data\n",
    "def analyze_stock(keyword):\n",
    "    # Load stock price data\n",
    "    stock_data = yf.download(keyword, start=\"2024-10-01\", end=datetime.today())[\"Close\"]\n",
    "    \n",
    "    # Load and filter news sentiment data\n",
    "    news_data = pd.read_csv(\"News_Data.csv\")\n",
    "    filtered_news = news_data[news_data[\"Text\"].str.contains(keyword, case=False, na=False)]\n",
    "    \n",
    "    # Aggregate sentiment by date\n",
    "    sentiment = (\n",
    "        filtered_news.groupby('Date')['Labeling_Sentiment']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .set_index(\"Date\")\n",
    "    )\n",
    "    \n",
    "    # Combine sentiment with stock prices\n",
    "    analysis_df = sentiment.copy()\n",
    "    analysis_df[\"Price\"] = stock_data\n",
    "    analysis_df.dropna(inplace=True)\n",
    "    \n",
    "    # Calculate indicators\n",
    "    analysis_df[\"Moving Average\"] = analysis_df[\"Price\"].rolling(window=5).mean()\n",
    "    analysis_df[\"RSI\"] = calculate_rsi(analysis_df[\"Price\"], window=5)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 16), gridspec_kw={\"height_ratios\": [3, 1, 1]})\n",
    "    \n",
    "    # Plot 1: Closing Price and Moving Average\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(analysis_df.index, analysis_df[\"Price\"], label=\"Closing Price\", color=\"blue\", linewidth=2)\n",
    "    ax1.plot(analysis_df.index, analysis_df[\"Moving Average\"], label=\"5-Day Moving Average\", color=\"orange\", linewidth=2)\n",
    "    ax1.set_title(f\"{keyword} Exchange Rate: Price and Moving Average\", fontsize=16, fontweight='bold')\n",
    "    ax1.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Price (USD)\", fontsize=14)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    Cursor(ax1, useblit=True, color='red', linewidth=1, linestyle='--')\n",
    "    \n",
    "    # Plot 2: RSI\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(analysis_df.index, analysis_df[\"RSI\"], label=\"RSI\", color=\"purple\", linewidth=2)\n",
    "    ax2.axhline(70, color=\"red\", linestyle=\"--\", label=\"Overbought (70)\")\n",
    "    ax2.axhline(30, color=\"green\", linestyle=\"--\", label=\"Oversold (30)\")\n",
    "    ax2.set_title(\"Relative Strength Index (RSI)\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax2.set_ylabel(\"RSI\", fontsize=12)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Sentiment Analysis\n",
    "    ax3 = axes[2]\n",
    "    ax3.plot(analysis_df.index, analysis_df[\"Labeling_Sentiment\"], label=\"Sentiment\", color=\"green\", linewidth=2)\n",
    "    sentiment_label = (\n",
    "        \"Neutral Sentiment (1)\" if 0.5 <= analysis_df[\"Labeling_Sentiment\"].iloc[-1] <= 1.5 else\n",
    "        \"Positive Sentiment (2)\" if analysis_df[\"Labeling_Sentiment\"].iloc[-1] > 1.5 else\n",
    "        \"Negative Sentiment (0)\"\n",
    "    )\n",
    "    ax3.axhline(0, color=\"black\", linestyle=\"--\", label=sentiment_label)\n",
    "    ax3.set_title(\"Sentiment Analysis\", fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax3.set_ylabel(\"Sentiment Score\", fontsize=12)\n",
    "    ax3.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Adjust layout and save the plot\n",
    "    plt.tight_layout()\n",
    "    output_path = \"output_plot.png\"\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n",
    "    return output_path\n",
    "\n",
    "# Gradio interface\n",
    "gui = gr.Interface(\n",
    "    fn=analyze_stock,\n",
    "    inputs=gr.Textbox(label=\"Enter Stock Name\"),\n",
    "    outputs=gr.Image(type=\"filepath\", label=\"Analysis Plot\"),\n",
    "    title=\"Stock Sentiment and Price Analysis\",\n",
    "    description=\"Analyze stock sentiment and price trends based on user input.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "gui.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd23af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
