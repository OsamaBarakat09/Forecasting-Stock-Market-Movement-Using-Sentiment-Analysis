{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db07c3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import gradio as gr\n",
    "import os, re, requests, nltk\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f635341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "Text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37925b16",
   "metadata": {},
   "source": [
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3143d",
   "metadata": {},
   "source": [
    "# Extract Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b4382-0f1d-40d3-9ec6-a0f12473926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for scraping Yahoo Finance news\n",
    "class HTMLLinkExtractor:\n",
    "    def __init__(self, max_links=162, timeout=10):\n",
    "        \"\"\"Initialize the HTMLLinkExtractor with max_links and timeout.\"\"\"\n",
    "        self.max_links = max_links\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def fetch_html(self, url):\n",
    "        \"\"\"Fetch the HTML content of a URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=self.timeout)\n",
    "            return BeautifulSoup(response.content, 'lxml')\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_links(self, soup):\n",
    "        \"\"\"Extract all valid HTTP links from the BeautifulSoup object.\"\"\"\n",
    "        links = set()\n",
    "        if soup:\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href')\n",
    "                if href and href.startswith('http'):\n",
    "                    links.add(href)\n",
    "        return links\n",
    "\n",
    "    def extract_html_links(self, url):\n",
    "        \"\"\"Extract HTML links starting from the given URL (for Yahoo Finance).\"\"\"\n",
    "        links = set()\n",
    "        soup = self.fetch_html(url)\n",
    "\n",
    "        # Extract initial links\n",
    "        if soup:\n",
    "            links.update(self.extract_links(soup))\n",
    "\n",
    "        # Iteratively fetch and extract links up to max_links\n",
    "        for link in list(links):\n",
    "            if len(links) >= self.max_links:\n",
    "                break\n",
    "            soup = self.fetch_html(link)\n",
    "            if soup:\n",
    "                links.update(self.extract_links(soup))\n",
    "\n",
    "        # Filter links ending with '.html'\n",
    "        return [link for link in links if link.endswith('.html')]\n",
    "\n",
    "# Function for scraping CNN or CNBC links\n",
    "def Scraping(url):\n",
    "    Links = []\n",
    "    response = requests.get(url)  # Send a request to the website\n",
    "    if response.status_code == 200:  # Check if the request was successful\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')  # Parse the HTML content of the page\n",
    "        links = soup.find_all('a', href=True)  # Find all anchor tags with 'href' attributes\n",
    "        for link in links or []:  # Extract and print the links\n",
    "            href = link['href']\n",
    "            if href.endswith(\"html\"):  # Filter out links to ensure they are valid URLs\n",
    "                Links.append(href)\n",
    "    return Links\n",
    "\n",
    "# Load the CSV and process links\n",
    "class LinkProcessor:\n",
    "    def __init__(self, csv_file, extractor):\n",
    "        \"\"\"Initialize the LinkProcessor with the CSV file and an HTMLLinkExtractor instance.\"\"\"\n",
    "        self.csv_file = csv_file\n",
    "        self.extractor = extractor\n",
    "\n",
    "    def process_links(self):\n",
    "        Old_Data = pd.read_csv(\"News_Data.csv\")\n",
    "        News_Links = pd.read_csv(\"News_Links.csv\")\n",
    "        update_file = {\"Link\": []}\n",
    "        update_Links = {\"Links\": []}\n",
    "        \"\"\"Process each link in the CSV file and extract HTML links.\"\"\"\n",
    "        links_df = pd.read_csv(self.csv_file)\n",
    "        \n",
    "        for link in tqdm(np.array(links_df[\"Links\"]), desc=\"Processing Links\", unit=\"link\"):\n",
    "            if str(link) not in News_Links[\"Links\"]:\n",
    "                if \"yahoo\" in str(link):  # Scrape Yahoo Finance using HTMLLinkExtractor\n",
    "                    html_links = self.extractor.extract_html_links(str(link))\n",
    "                    for i in html_links:\n",
    "                        update_file[\"Link\"].append(i)\n",
    "                        update_Links[\"Links\"].append(i)\n",
    "                elif \"cnn\" in str(link):  # Scrape CNN or CNBC using Scraping function\n",
    "                    html_links = Scraping(str(link))\n",
    "                    for i in html_links:\n",
    "                        update_file[\"Link\"].append(f\"https://edition.cnn.com{i}\")\n",
    "                        update_Links[\"Links\"].append(i)\n",
    "                elif \"cnbc\" in str(link):\n",
    "                    html_links = Scraping(str(link))\n",
    "                    for i in html_links:\n",
    "                        update_file[\"Link\"].append(i)\n",
    "                        update_Links[\"Links\"].append(i)\n",
    "\n",
    "        update_file = pd.DataFrame(update_file)\n",
    "        update_Links = pd.DataFrame(update_Links)\n",
    "        \n",
    "        add_links = pd.concat([Old_Data, update_file]).drop_duplicates(subset=\"Link\")\n",
    "        Links = pd.concat([News_Links, update_Links]).drop_duplicates(subset=\"Links\")\n",
    "        \n",
    "        add_links.to_csv(\"News_Data.csv\", index=False)\n",
    "        Links.to_csv(\"News_Links.csv\", index=False)\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    extractor = HTMLLinkExtractor(max_links=162, timeout=10)\n",
    "    processor = LinkProcessor(\"News_Links.csv\", extractor)\n",
    "    processor.process_links()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd51e51",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fefccc",
   "metadata": {},
   "source": [
    "# Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b50598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleExtractor:\n",
    "    def __init__(self, url, target_class):\n",
    "        self.url = url\n",
    "        self.target_class = target_class\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    def fetch_text(self):\n",
    "        try:\n",
    "            response = requests.get(self.url, headers=self.headers)\n",
    "            response.raise_for_status()  # Raise an error for unsuccessful requests\n",
    "        except requests.exceptions.RequestException:\n",
    "            return \"0\"\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        elements = soup.find_all(class_=self.target_class)\n",
    "\n",
    "        if not elements:\n",
    "            return \"0\"\n",
    "\n",
    "        # Extract and return text content\n",
    "        result = [element.get_text(strip=True) for element in elements]\n",
    "\n",
    "        return result if result else []\n",
    "\n",
    "class NewsProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "\n",
    "    def process_articles(self):\n",
    "        # Use tqdm to add a progress bar\n",
    "        for i in tqdm(range(len(self.data)), desc=\"Processing Articles\"):\n",
    "            link = self.data[\"Link\"][i]\n",
    "            text = self.data[\"Text\"][i]\n",
    "\n",
    "            if isinstance(text, float):  # Check if Text is a NaN value (float)\n",
    "                if \"yahoo\" in link:\n",
    "                    extractor = ArticleExtractor(link, \"article-wrap no-bb\")\n",
    "                elif \"cnn\" in link:\n",
    "                    extractor = ArticleExtractor(link, \"article__content\")\n",
    "                elif \"cnbc\" in link:\n",
    "                    extractor = ArticleExtractor(link, \"ArticleBody-articleBody\")\n",
    "                else:\n",
    "                    continue  # Skip if no match for source\n",
    "                \n",
    "                # Extract text and update the dataframe\n",
    "                extracted_text = extractor.fetch_text()\n",
    "                self.data.loc[i, \"Text\"] = extracted_text\n",
    "\n",
    "    def save_data(self, output_path):\n",
    "        self.data.to_csv(output_path, index=False)\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"News_Data.csv\"\n",
    "output_path = \"News_Data.csv\"\n",
    "\n",
    "news_processor = NewsProcessor(file_path)\n",
    "news_processor.process_articles()\n",
    "news_processor.save_data(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe128a0f",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee154c97",
   "metadata": {},
   "source": [
    "# Clean Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "\n",
    "# Define the preprocessor function\n",
    "def preprocessor(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]*>', '', str(text))\n",
    "    \n",
    "    # Find emoticons\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # Remove non-word characters and convert to lowercase, then add emoticons back\n",
    "    text = (re.sub(r'[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the preprocessor to the \"Text\" column\n",
    "Text[\"Text\"] = Text[\"Text\"].apply(preprocessor)\n",
    "\n",
    "# Save the processed data back to CSV\n",
    "Text.to_csv(\"News_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecb5e3",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c047950-27bf-4bb1-a9a3-225d83a83c11",
   "metadata": {},
   "source": [
    "# Filter data to contain just finance news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a493e1-3ca1-45e6-a230-9e3a6bf1a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceTextClassifier:\n",
    "    def __init__(self, finance_keywords_path: str = \"Finance_Key_Words.csv\"):\n",
    "        \"\"\"\n",
    "        Initialize the classifier with financial keywords and regex patterns.\n",
    "        \n",
    "        Args:\n",
    "            finance_keywords_path (str): Path to the CSV file containing financial keywords.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # Load financial keywords\n",
    "        self.finance_keywords = self._load_finance_keywords(finance_keywords_path)\n",
    "        \n",
    "        # Precompile regex patterns for faster matching\n",
    "        self.finance_patterns = [\n",
    "            re.compile(r'\\$\\d+(\\.\\d+)?'),  # Currency amounts\n",
    "            re.compile(r'\\d+%'),  # Percentage\n",
    "            re.compile(r'\\b\\d+(\\.\\d+)?\\s*(million|billion|trillion)\\b'),  # Large number scales\n",
    "        ]\n",
    "\n",
    "        # Precompute stopwords for faster filtering\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    def _load_finance_keywords(self, path: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Load financial keywords from a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            path (str): Path to the CSV file.\n",
    "        \n",
    "        Returns:\n",
    "            Set[str]: A set of financial keywords.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return set(pd.read_csv(path)[\"Key\"].str.lower().dropna().unique())\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading financial keywords: {e}\")\n",
    "            return set()\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess the text by converting to lowercase, removing special characters,\n",
    "        and filtering out stopwords.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to preprocess.\n",
    "        \n",
    "        Returns:\n",
    "            str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "        words = text.split()\n",
    "        words = [word for word in words if word not in self.stopwords]  # Remove stopwords\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def _count_finance_keywords(self, text: str) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Count occurrences of finance-related keywords in the text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Preprocessed text.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, int]: A dictionary of financial keywords and their counts.\n",
    "        \"\"\"\n",
    "        keyword_counts = defaultdict(int)\n",
    "        for word in text.split():\n",
    "            if word in self.finance_keywords:\n",
    "                keyword_counts[word] += 1\n",
    "        return dict(keyword_counts)\n",
    "\n",
    "    def _check_finance_patterns(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check for financial regex patterns in the text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: A list of matched financial patterns.\n",
    "        \"\"\"\n",
    "        matched_patterns = []\n",
    "        for pattern in self.finance_patterns:\n",
    "            matches = pattern.findall(text)\n",
    "            if matches:\n",
    "                matched_patterns.extend(matches)\n",
    "        return matched_patterns\n",
    "\n",
    "    def classify_text(self, text: str, threshold: float = 0.1) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Classify if text is finance-related.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to classify.\n",
    "            threshold (float): Minimum keyword ratio to consider finance-related.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, any]: Classification results with details.\n",
    "        \"\"\"\n",
    "        # Preprocess text\n",
    "        processed_text = self._preprocess_text(text)\n",
    "        \n",
    "        # Count words\n",
    "        total_words = len(processed_text.split())\n",
    "        \n",
    "        # Count finance keywords\n",
    "        keyword_counts = self._count_finance_keywords(processed_text)\n",
    "        keyword_total_count = sum(keyword_counts.values())\n",
    "        \n",
    "        # Check financial patterns\n",
    "        pattern_matches = self._check_finance_patterns(text)\n",
    "        \n",
    "        # Calculate finance keyword ratio\n",
    "        keyword_ratio = keyword_total_count / total_words if total_words > 0 else 0\n",
    "        \n",
    "        # Determine classification\n",
    "        is_finance_related = keyword_ratio >= threshold or len(pattern_matches) > 0\n",
    "        \n",
    "        return {\n",
    "            'is_finance_related': is_finance_related,\n",
    "            'keyword_ratio': keyword_ratio,\n",
    "            'keyword_counts': keyword_counts,\n",
    "            'pattern_matches': pattern_matches,\n",
    "            'total_words': total_words\n",
    "        }\n",
    "\n",
    "\n",
    "    \n",
    "Text = pd.read_csv(\"News_Data.csv\")\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = FinanceTextClassifier()\n",
    "\n",
    "# Filter finance-related texts\n",
    "filtered_results = []\n",
    "for index, row in Text.iterrows():\n",
    "    result = classifier.classify_text(row[\"Text\"])\n",
    "    if result['is_finance_related']:\n",
    "        filtered_results.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "filtered_df = pd.DataFrame(filtered_results)\n",
    "filtered_df.to_csv(\"News_Data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa56d4b-377a-4f3a-bee3-6d9bb1125886",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff90a0-1df4-4e2d-bf75-882c4fe8347b",
   "metadata": {},
   "source": [
    "# Remove duplicates patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4407cc-6ed8-4b50-b1c1-e76cb4822bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternExtractor:\n",
    "    def __init__(self, file_path, min_words=10, occurrence_threshold=4):\n",
    "        self.file_path = file_path\n",
    "        self.min_words = min_words\n",
    "        self.occurrence_threshold = occurrence_threshold\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads dataset from CSV file.\"\"\"\n",
    "        return pd.read_csv(self.file_path)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Cleans text by removing extra spaces, punctuation, and converting to lowercase.\"\"\"\n",
    "        text = re.sub('<[^>]*>', '', str(text))\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\(|\\)|D|P)', text)\n",
    "        text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "        return text.lower()\n",
    "\n",
    "    def extract_patterns_from_text(self, text):\n",
    "        \"\"\"Extracts patterns from a single article.\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        patterns = set()\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence)\n",
    "            for i in range(len(words) - self.min_words + 1):\n",
    "                patterns.add(' '.join(words[i:i + self.min_words]))\n",
    "        return patterns\n",
    "\n",
    "    def find_common_patterns(self):\n",
    "        \"\"\"Finds common patterns across articles.\"\"\"\n",
    "        pattern_counts = defaultdict(int)\n",
    "        for article in tqdm(self.data[\"Text\"], desc=\"Processing Articles\"):\n",
    "            preprocessed_text = self.preprocess_text(article)\n",
    "            patterns = self.extract_patterns_from_text(preprocessed_text)\n",
    "            for pattern in patterns:\n",
    "                pattern_counts[pattern] += 1\n",
    "        return {pattern: count for pattern, count in pattern_counts.items() if count > 1}\n",
    "\n",
    "    def remove_patterns_from_text(self, text, patterns):\n",
    "        \"\"\"Removes patterns from text.\"\"\"\n",
    "        for pattern in patterns:\n",
    "            text = re.sub(re.escape(pattern), \"\", text)\n",
    "        return text\n",
    "\n",
    "    def update_data(self):\n",
    "        \"\"\"Updates data by removing high-frequency patterns.\"\"\"\n",
    "        common_patterns = self.find_common_patterns()\n",
    "        high_frequency_patterns = [pattern for pattern, count in common_patterns.items() if count > self.occurrence_threshold]\n",
    "        self.data[\"Text\"] = self.data[\"Text\"].apply(lambda x: self.remove_patterns_from_text(x, high_frequency_patterns))\n",
    "        self.data.to_csv(\"News_Data_Updated.csv\", index=False)\n",
    "        print(\"Updated News_Data_Updated.csv saved successfully!\")\n",
    "\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    extractor = PatternExtractor(\"News_Data.csv\")\n",
    "    extractor.update_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9ac76-4e10-415a-a002-6a852c2728fa",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c6df4",
   "metadata": {},
   "source": [
    "# Extract Date And Formating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37850f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 8760/8760 [00:00<00:00, 73887.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████| 8760/8760 [00:02<00:00, 4185.51it/s]\n"
     ]
    }
   ],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.text_df = pd.read_csv(csv_file)\n",
    "\n",
    "    def get_text(self, url, target_class=None):\n",
    "        try:\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()  # Raise an error for unsuccessful requests\n",
    "        except requests.exceptions.RequestException:\n",
    "            return \"0\"\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        elements = soup.find_all(class_=target_class)\n",
    "        if not elements:\n",
    "            return \"0\"\n",
    "\n",
    "        return [element.get_text(strip=True) for element in elements] if target_class else soup.get_text(strip=True)\n",
    "\n",
    "    def is_valid_date(self, date_str):\n",
    "        try:\n",
    "            parse(date_str)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def extract_elements_by_class(self, url, target_class):\n",
    "        result = self.get_text(url, target_class)\n",
    "        if result != \"0\" and self.is_valid_date(result[0]):\n",
    "            return result[0]\n",
    "        return \"0\"\n",
    "\n",
    "    def get_publication_date(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            date_meta_tags = [\n",
    "                {\"name\": \"pubdate\"}, {\"name\": \"publish-date\"}, {\"name\": \"creation-date\"},\n",
    "                {\"name\": \"date\"},{\"property\": \"article:published_time\"},\n",
    "                {\"property\": \"og:article:published_time\"}\n",
    "            ]\n",
    "\n",
    "            for tag in date_meta_tags:\n",
    "                date_tag = soup.find(\"meta\", tag)\n",
    "                if date_tag and date_tag.get(\"content\"):\n",
    "                    return date_tag['content'][:10]\n",
    "\n",
    "            possible_date_tags = soup.find_all([\"time\", \"span\", \"p\"])\n",
    "            for tag in possible_date_tags:\n",
    "                if tag.has_attr(\"datetime\"):\n",
    "                    return tag['datetime'][:10]\n",
    "                elif \"published\" in tag.get(\"class\", []) or \"date\" in tag.get(\"class\", []):\n",
    "                    return tag.text.strip()[:10]\n",
    "\n",
    "            text_content = soup.get_text()\n",
    "            date_patterns = [\n",
    "                r'\\b\\d{4}-\\d{2}-\\d{2}\\b', r'\\b\\d{2}/\\d{2}/\\d{4}\\b', r'\\b\\d{1,2} \\w{3,9} \\d{4}\\b'\n",
    "            ]\n",
    "            for pattern in date_patterns:\n",
    "                match = re.search(pattern, text_content)\n",
    "                if match:\n",
    "                    return match.group()[:10]\n",
    "\n",
    "            return \"0\"\n",
    "        except requests.RequestException:\n",
    "            return \"0\"\n",
    "\n",
    "    def extract_date_as_date_type(self, text):\n",
    "        date_pattern = r'\\b(\\w+)\\s(\\d{1,2}),\\s(\\d{4})\\b'\n",
    "        match = re.search(date_pattern, text)\n",
    "\n",
    "        if match:\n",
    "            month_name, day, year = match.groups()\n",
    "            date_obj = datetime.strptime(f\"{month_name} {day} {year}\", \"%B %d %Y\").date()\n",
    "            return date_obj\n",
    "        return \"0\"\n",
    "\n",
    "    def convert_date(self, date_str):\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            return date_obj.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        formats = ['%m/%d/%Y', '%d-%b-%y', '%d-%b-%Y', '%a, %b %d, %Y','%B %d, %Y,',\"%B %d, %Y\", '%a, %b %d, %Y,', \n",
    "                   \"%b %d, %Y\", \"%b %d, %Y,\",\"%d-%b-%y\", \"%a, %b %d, %Y, %I:%M %p\", \"%a, %b %d, %Y\"]\n",
    "\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, fmt)\n",
    "                return date_obj.strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        return None\n",
    "\n",
    "    def extract_dates(self):\n",
    "        for i in tqdm(range(len(self.text_df)), ncols=100):\n",
    "            try:\n",
    "                link = self.text_df[\"Link\"][i]\n",
    "                current_date = self.text_df[\"Date\"][i]\n",
    "\n",
    "                if current_date == \"0\" or pd.isna(current_date):\n",
    "                    if \"abcnews\" in link:\n",
    "                        self.text_df.loc[i, \"Date\"] = self.extract_elements_by_class(link, \"VZTD mLASH gpiba\".strip())\n",
    "\n",
    "                    elif \"yahoo\" in link:\n",
    "                        self.text_df.loc[i, \"Date\"] = self.extract_elements_by_class(link, \"byline-attr-meta-time\".strip())\n",
    "\n",
    "                    elif \"cnn\" in link:\n",
    "                        timestamp = self.get_text(link, \"timestamp vossi-timestamp\")\n",
    "                        self.text_df.loc[i, \"Date\"] = self.extract_date_as_date_type(timestamp[0]) if timestamp else \"0\"\n",
    "\n",
    "                    elif \"coindesk\" in link:\n",
    "                        content = self.get_text(link, \"Noto_Sans_xs_Sans-400-xs flex gap-4 text-charcoal-600 flex-col md:flex-row\")[0][:12]\n",
    "                        self.text_df.loc[i, \"Date\"] = content\n",
    "\n",
    "                    elif \"teslarati\" in link:\n",
    "                        content = self.get_text(link, \"post-date updated\")[0]\n",
    "                        self.text_df.loc[i, \"Date\"] = content\n",
    "\n",
    "                    elif \"cnbc\" in link:\n",
    "                        content = self.get_publication_date(link)\n",
    "                        self.text_df.loc[i, \"Date\"] = content\n",
    "\n",
    "                    else:\n",
    "                        self.text_df.loc[i, \"Date\"] = current_date\n",
    "            except Exception as e:\n",
    "                pass  # Log or handle the exception if needed\n",
    "\n",
    "    def process_and_save(self):\n",
    "        self.extract_dates()\n",
    "        for i in tqdm(range(len(self.text_df)), ncols=100):\n",
    "            try:\n",
    "                self.text_df.loc[i, \"Date\"] = self.convert_date(self.text_df[\"Date\"][i])\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.text_df.to_csv(self.csv_file, index=False)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "scraper = NewsScraper(\"News_Data.csv\")\n",
    "scraper.process_and_save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8ec8e",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb2b7f",
   "metadata": {},
   "source": [
    "# Labeling Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645ecc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 8760/8760 [00:00<00:00, 157060.49it/s]\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Osama Barakat\\.cache\\huggingface\\hub\\models--yiyanghkust--finbert-tone. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "class SentimentAnalyzer:   \n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.text_df = pd.read_csv(csv_file)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "        self.encoder = LabelEncoder()\n",
    "        \n",
    "    def get_sentiment(self, text):\n",
    "        \"\"\"Get sentiment from text using FinBERT model.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "        probabilities = softmax(outputs.logits, dim=1)\n",
    "        sentiment_labels = [\"neutral\", \"positive\", \"negative\"]\n",
    "        return sentiment_labels[probabilities.argmax()]\n",
    "\n",
    "    def process_sentiments(self):\n",
    "        \"\"\"Process and update sentiment labels and encoding in the DataFrame.\"\"\"\n",
    "        for i in tqdm(range(len(self.text_df[\"Sentiment\"])), ncols=100):\n",
    "            try:\n",
    "                if pd.isna(self.text_df[\"Sentiment\"][i]):  # Only update if sentiment is missing\n",
    "                    sentiment = self.get_sentiment(self.text_df[\"Text\"][i])\n",
    "                    self.text_df.loc[i, \"Sentiment\"] = sentiment\n",
    "            except Exception as e:\n",
    "                pass  # Handle or log the exception if necessary\n",
    "\n",
    "    def encode_sentiments(self):\n",
    "        \"\"\"Encode the sentiment column into numerical labels.\"\"\"\n",
    "        self.text_df[\"Labeling_Sentiment\"] = self.encoder.fit_transform(self.text_df[\"Sentiment\"])\n",
    "\n",
    "    def save_to_csv(self):\n",
    "        \"\"\"Save the updated DataFrame back to CSV.\"\"\"\n",
    "        self.text_df.to_csv(self.csv_file, index=False)\n",
    "\n",
    "    def run_analysis(self):\n",
    "        \"\"\"Run the full sentiment analysis process.\"\"\"\n",
    "        self.process_sentiments()\n",
    "        self.encode_sentiments()\n",
    "        self.save_to_csv()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "sentiment_analyzer = SentimentAnalyzer(\"News_Data.csv\")\n",
    "sentiment_analyzer.run_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da4742",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183fa88",
   "metadata": {},
   "source": [
    "# Cleaning data & Formating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38200615-fb79-4159-9821-385be1f96515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, csv_file, is_url_function):\n",
    "        \"\"\"\n",
    "        Initialize the DataCleaner class.\n",
    "        \n",
    "        Parameters:\n",
    "        - csv_file: Path to the CSV file to process.\n",
    "        - is_url_function: A function to validate URLs.\n",
    "        \"\"\"\n",
    "        self.csv_file = csv_file\n",
    "        self.is_url = is_url_function\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def filter_data(self):\n",
    "        \"\"\"\n",
    "        Filter and clean the dataset:\n",
    "        - Remove duplicate rows based on 'Link' and 'Text'.\n",
    "        - Keep rows with valid URLs in 'Link'.\n",
    "        - Exclude rows where 'Text' or 'Date' is invalid or missing.\n",
    "        \"\"\"\n",
    "        self.data = self.data.drop_duplicates(subset=[\"Link\", \"Text\"])\n",
    "        self.data = self.data[self.data[\"Link\"].apply(self.is_url).fillna(False)]\n",
    "        self.data = self.data[self.data[\"Text\"] != \"0\"]\n",
    "        self.data = self.data[self.data[\"Date\"] != \"0\"]\n",
    "        self.data = self.data.dropna(subset=[\"Date\"])  # Ensure 'Date' column is not None\n",
    "\n",
    "    def save_filtered_data(self, output_file=None):\n",
    "        \"\"\"\n",
    "        Save the cleaned and filtered data to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        - output_file: Optional; path to save the cleaned data. Defaults to the original file.\n",
    "        \"\"\"\n",
    "        output_file = output_file or self.csv_file\n",
    "        self.data.to_csv(output_file, index=False)\n",
    "\n",
    "    def create_refresh_dict(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary of the filtered data.\n",
    "        \n",
    "        Returns:\n",
    "        A dictionary with keys 'Link', 'Text', 'Sentiment', 'Date', and 'Labeling_Sentiment'.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Link\": self.data[\"Link\"].tolist(),\n",
    "            \"Text\": self.data[\"Text\"].tolist(),\n",
    "            \"Sentiment\": self.data[\"Sentiment\"].tolist(),\n",
    "            \"Date\": self.data[\"Date\"].tolist(),\n",
    "            \"Labeling_Sentiment\": self.data[\"Labeling_Sentiment\"].tolist(),\n",
    "        }\n",
    "\n",
    "    def run_cleaning(self, output_file=None):\n",
    "        \"\"\"\n",
    "        Execute the complete cleaning process:\n",
    "        - Filter the data.\n",
    "        - Save the cleaned data.\n",
    "        - Return the refresh dictionary.\n",
    "        \n",
    "        Parameters:\n",
    "        - output_file: Optional; path to save the cleaned data.\n",
    "        \n",
    "        Returns:\n",
    "        The refresh dictionary.\n",
    "        \"\"\"\n",
    "        self.filter_data()\n",
    "        self.save_filtered_data(output_file)\n",
    "        return self.create_refresh_dict()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def is_url_example(url):\n",
    "    # Replace this with your `is_url` function implementation\n",
    "    return True  # Simplified for example\n",
    "\n",
    "data_cleaner = DataCleaner(\"News_Data.csv\", is_url_function=is_url_example)\n",
    "refresh_dict = data_cleaner.run_cleaning(\"News_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1633bbad",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e19657-131a-4b18-a904-7a66c2569cd3",
   "metadata": {},
   "source": [
    "# Display forecasting Using Moveing Avereage & RSI & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc494e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate RSI\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data.diff(1)\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Function to analyze stock data\n",
    "def analyze_stock(keyword):\n",
    "    # Load stock price data\n",
    "    stock_data = yf.download(keyword, start=\"2024-10-01\", end=datetime.today())[\"Close\"]\n",
    "    \n",
    "    # Load and filter news sentiment data\n",
    "    news_data = pd.read_csv(\"News_Data.csv\")\n",
    "    filtered_news = news_data[news_data[\"Text\"].str.contains(keyword, case=False, na=False)]\n",
    "    \n",
    "    # Aggregate sentiment by date\n",
    "    sentiment = (\n",
    "        filtered_news.groupby('Date')['Labeling_Sentiment']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .set_index(\"Date\")\n",
    "    )\n",
    "    \n",
    "    # Combine sentiment with stock prices\n",
    "    analysis_df = sentiment.copy()\n",
    "    analysis_df[\"Price\"] = stock_data\n",
    "    analysis_df.dropna(inplace=True)\n",
    "    \n",
    "    # Calculate indicators\n",
    "    analysis_df[\"Moving Average\"] = analysis_df[\"Price\"].rolling(window=5).mean()\n",
    "    analysis_df[\"RSI\"] = calculate_rsi(analysis_df[\"Price\"], window=5)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 16), gridspec_kw={\"height_ratios\": [3, 1, 1]})\n",
    "    \n",
    "    # Plot 1: Closing Price and Moving Average\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(analysis_df.index, analysis_df[\"Price\"], label=\"Closing Price\", color=\"blue\", linewidth=2)\n",
    "    ax1.plot(analysis_df.index, analysis_df[\"Moving Average\"], label=\"5-Day Moving Average\", color=\"orange\", linewidth=2)\n",
    "    ax1.set_title(f\"{keyword} Exchange Rate: Price and Moving Average\", fontsize=16, fontweight='bold')\n",
    "    ax1.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Price (USD)\", fontsize=14)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    Cursor(ax1, useblit=True, color='red', linewidth=1, linestyle='--')\n",
    "    \n",
    "    # Plot 2: RSI\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(analysis_df.index, analysis_df[\"RSI\"], label=\"RSI\", color=\"purple\", linewidth=2)\n",
    "    ax2.axhline(70, color=\"red\", linestyle=\"--\", label=\"Overbought (70)\")\n",
    "    ax2.axhline(30, color=\"green\", linestyle=\"--\", label=\"Oversold (30)\")\n",
    "    ax2.set_title(\"Relative Strength Index (RSI)\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax2.set_ylabel(\"RSI\", fontsize=12)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 3: Sentiment Analysis\n",
    "    ax3 = axes[2]\n",
    "    ax3.plot(analysis_df.index, analysis_df[\"Labeling_Sentiment\"], label=\"Sentiment\", color=\"green\", linewidth=2)\n",
    "    sentiment_label = (\n",
    "        \"Neutral Sentiment (1)\" if 0.5 <= analysis_df[\"Labeling_Sentiment\"].iloc[-1] <= 1.5 else\n",
    "        \"Positive Sentiment (2)\" if analysis_df[\"Labeling_Sentiment\"].iloc[-1] > 1.5 else\n",
    "        \"Negative Sentiment (0)\"\n",
    "    )\n",
    "    ax3.axhline(0, color=\"black\", linestyle=\"--\", label=sentiment_label)\n",
    "    ax3.set_title(\"Sentiment Analysis\", fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax3.set_ylabel(\"Sentiment Score\", fontsize=12)\n",
    "    ax3.grid(True, linestyle='--', alpha=0.6)\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Adjust layout and save the plot\n",
    "    plt.tight_layout()\n",
    "    output_path = \"output_plot.png\"\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n",
    "    return output_path\n",
    "\n",
    "# Gradio interface\n",
    "gui = gr.Interface(\n",
    "    fn=analyze_stock,\n",
    "    inputs=gr.Textbox(label=\"Enter Stock Name\"),\n",
    "    outputs=gr.Image(type=\"filepath\", label=\"Analysis Plot\"),\n",
    "    title=\"Stock Sentiment and Price Analysis\",\n",
    "    description=\"Analyze stock sentiment and price trends based on user input.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "gui.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
